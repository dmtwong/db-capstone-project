summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75)))
# pass multiple values to percentile(), we can call another Hive function called array()
# In this case array() would work similarly to Rs list()
summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75)))
# use the explode() function to separate Sparks array value results into their own record.
summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75))) %>%
mutate(mpg_percentile = explode(mpg_percentile))
summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75))) %>%
mutate(mpg_percentile = explode(mpg_percentile)) %>% show_query()
ml_corr(cars)
class(ml_corr(cars))
ml_corr(cars) %>% show_query()
ml_corr(cars)
library(corrr)
correlate(cars, use = "pairwise.complete.obs", method = "pearson")
# ml_corr(cars) %>% show_query()
ml_corr(cars)
# can pipe the results to other corrr functions. For example, the shave() function turns all of the duplicated results into NAs
correlate(cars, use = "pairwise.complete.obs", method = "pearson") %>%
shave() %>%
rplot()
library(ggplot2)
ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col()
car_group <- cars %>%
group_by(cyl) %>%
summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
collect() %>%
print()
ggplot(aes(as.factor(cyl), mpg), data = car_group) +
geom_col(fill = "#999999") + coord_flip()
# dbplot_histogram() function makes Spark calculate the bins and the count per bin and outputs a ggplot object
library(dbplot)
cars %>%
dbplot_histogram(mpg, binwidth = 3) +
labs(title = "MPG Distribution",
subtitle = "Histogram over miles per gallon")
ggplot(aes(mpg, wt), data = mtcars) +
geom_point()
# for scatter plots, no amount of “pushing the computation” to Spark will help with this problem because the data must be plotted in individual dots.
# The best alternative is to find a plot type that represents the x/y relationship and concentration in a way that it is easy to perceive and to plot
# raster plot might be the best answer.
dbplot_raster(cars, mpg, wt, resolution = 16)
cars %>%
ml_linear_regression(mpg ~ .) %>%
summary()
cars %>%
ml_linear_regression(mpg ~ .) %>%
summary() %>% show_query()
cars %>%
ml_linear_regression(mpg ~ hp + cyl) %>%
summary()
cars %>%
ml_generalized_linear_regression(mpg ~ hp + cyl) %>%
summary()
cached_cars <- cars %>%
mutate(cyl = paste0("cyl_", cyl)) %>%
compute("cached_cars")
cached_cars
cached_cars %>%
ml_linear_regression(mpg ~ .) %>%
summary()
spark_disconnect(sc)
download.file(
"https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip",
"okcupid.zip")
unzip("okcupid.zip", exdir = "data")
?unlink
unlink("okcupid.zip")
install.packages("ggmosaic")
install.packages("forcats")
install.packages("FactoMineR")
install.packages("FactoMineR")
#   "https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip",
#   "okcupid.zip")
#
# unzip("okcupid.zip", exdir = "data")
# ?unlink
# unlink("okcupid.zip")
#
# install.packages("ggmosaic")
# install.packages("forcats")
# install.packages("FactoMineR")
library(sparklyr)
library(ggplot2)
library(dbplot)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.3")
sc <- spark_connect(master = "local")
tmp <- read.csv("data/profiles.csv")
object.size('tmp')
object.size(tmp)
print(object.size(tmp))
?print
# ?print
?object.size
print(object.size(tmp), units = "mb")
format(object.size(tmp), units = "mb")
format(tmp, units = "mb")
# format(tmp, units = "mb")
# ?print
# ?object.size
rm(list = 'tmp')
okc <- spark_read_csv(
sc,
"data/profiles.csv",
escape = "\"",
memory = FALSE,
options = list(multiline = TRUE)
) %>%
mutate(
height = as.numeric(height),
income = ifelse(income == "-1", NA, as.numeric(income))
) %>%
mutate(sex = ifelse(is.na(sex), "missing", sex)) %>%
mutate(drinks = ifelse(is.na(drinks), "missing", drinks)) %>%
mutate(drugs = ifelse(is.na(drugs), "missing", drugs)) %>%
mutate(job = ifelse(is.na(job), "missing", job))
spark_web(sc)
?spark_read_csv
okc
glimpse(okc)
okc <- okc %>%
mutate(
not_working = ifelse(job %in% c("student", "unemployed", "retired"), 1 , 0)
)
okc %>%
group_by(not_working) %>%
tally()
data_splits <- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42)
okc_train <- data_splits$training
okc_test <- data_splits$testing
glimpse(okc_test)
glimpse(okc_train)
okc_train %>%
group_by(not_working) %>%
tally() %>%
mutate(frac = n / sum(n))
okc %>%
group_by(not_working) %>%
tally() %>%
mutate(frac = n / sum(n))
sdf_describe(okc_train, cols = c("age", "income"))
dbplot_histogram(okc_train, age)
dbplot_histogram(okc_train, income)
prop_data <- okc_train %>%
mutate(religion = regexp_extract(religion, "^\\\\w+", 0)) %>%
group_by(religion, not_working) %>%
tally() %>%
group_by(religion) %>%
summarize(
count = sum(n),
prop = sum(not_working * n) / sum(n)
) %>%
mutate(se = sqrt(prop * (1 - prop) / count)) %>%
collect()
prop_data
prop_data %>% arrange(desc(prop))
prop_data %>%
ggplot(aes(x = religion, y = prop)) + geom_point(size = 2) +
geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se),
width = .1) +
geom_hline(yintercept = sum(prop_data$prop * prop_data$count) /
sum(prop_data$count))
prop_data %>%
ggplot(aes(x = religion, y = prop)) + geom_point(size = 2) +
geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se),
width = .1) +
geom_hline(yintercept = sum(prop_data$prop * prop_data$count) /
sum(prop_data$count)) +
coord_flip()
contingency_tbl <- okc_train %>%
sdf_crosstab("drinks", "drugs") %>%
collect()
contingency_tbl
library(ggmosaic)
library(forcats)
library(tidyr)
contingency_tbl %>%
rename(drinks = drinks_drugs) %>%
gather("drugs", "count", missing:sometimes) %>%
mutate(
drinks = as_factor(drinks) %>%
fct_relevel("missing", "not at all", "rarely", "socially",
"very often", "desperately"),
drugs = as_factor(drugs) %>%
fct_relevel("missing", "never", "sometimes", "often")
) %>%
ggplot() +
geom_mosaic(aes(x = product(drinks, drugs), fill = drinks,
weight = count))
?gather
contingency_tbl %>%
rename(drinks = drinks_drugs) %>%
gather("drugs", "count", missing:sometimes) %>%
mutate(
drinks = as_factor(drinks) %>%
fct_relevel("missing", "not at all", "rarely", "socially",
"very often", "desperately"),
drugs = as_factor(drugs) %>%
fct_relevel("missing", "never", "sometimes", "often")
) %>%
ggplot() +
geom_mosaic(aes(x = product(drinks, drugs), fill = drinks,
weight = count))
contingency_tbl %>%
rename(drinks = drinks_drugs) %>%
gather("drugs", "count", missing:sometimes) %>%
mutate(
drinks = as_factor(drinks) %>%
fct_relevel("missing", "not at all", "rarely", "socially",
"very often", "desperately"),
drugs = as_factor(drugs) %>%
fct_relevel("missing", "never", "sometimes", "often")
) %>%
ggplot() +
geom_mosaic(aes(x = product(drinks, drugs), fill = drinks,
weight = count)) + coord_flip()
contingency_tbl %>%
rename(drinks = drinks_drugs) %>%
gather("drugs", "count", missing:sometimes) %>%
mutate(
drinks = as_factor(drinks) %>%
fct_relevel("missing", "not at all", "rarely", "socially",
"very often", "desperately"),
drugs = as_factor(drugs) %>%
fct_relevel("missing", "never", "sometimes", "often")
) %>%
ggplot() +
geom_mosaic(aes(x = product(drinks, drugs), fill = drinks,
weight = count))
contingency_tbl
dd_obj <- contingency_tbl %>%
tibble::column_to_rownames(var = "drinks_drugs") %>%
FactoMineR::CA(graph = FALSE)
class(dd_obj)
dd_obj
dd_drugs <-
dd_obj$row$coord %>%
as.data.frame() %>%
mutate(
label = gsub("_", " ", rownames(dd_obj$row$coord)),
Variable = "Drugs"
)
dd_drugs
dd_drinks <-
dd_obj$col$coord %>%
as.data.frame() %>%
mutate(
label = gsub("_", " ", rownames(dd_obj$col$coord)),
Variable = "Alcohol"
)
dd_drinks
ca_coord <- rbind(dd_drugs, dd_drinks)
ca_coord
ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`,
col = Variable)) +
geom_vline(xintercept = 0) +
geom_hline(yintercept = 0) +
geom_text(aes(label = label)) +
coord_equal()
scale_values <- okc_train %>%
summarize(
mean_age = mean(age),
sd_age = sd(age)
) %>%
collect()
scale_values
okc_train <- okc_train %>%
mutate(scaled_age = (age - !!scale_values$mean_age) /
!!scale_values$sd_age)
dbplot_histogram(okc_train, scaled_age)
okc_train %>%
group_by(ethnicity) %>%
tally()
ethnicities <- c("asian", "middle eastern", "black", "native american", "indian",
"pacific islander", "hispanic / latin", "white", "other")
ethnicity_vars <- ethnicities %>%
purrr::map(~ expr(ifelse(like(ethnicity, !!.x), 1, 0))) %>%
purrr::set_names(paste0("ethnicity_", gsub("\\s|/", "", ethnicities)))
ethnicity_vars
okc_train %>%
select(starts_with("ethnicity_")) %>%
glimpse()
okc_train <- mutate(okc_train, !!!ethnicity_vars)
okc_train %>%
select(starts_with("ethnicity_")) %>%
glimpse()
system("java -version")
system("SPARK_HOME")
system("SPARK -version")
system("spark -version")
spark_installed_versions()
library(sparklyr)
system("spark -version")
spark_installed_versions()
system("spark_home_dir")
spark_home_dir
spark_home_dir()
library(dplyr)
?summarise_all
summarise_all
?explode
sparklyr::explode
sparklyr:::explode
library(explode)
library(sparklyr)
explode
??explode
sparklyr.nested::sdf_explode
library(ggplot2)
ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col()
random(1:50, 7)
sample(1:50, 7)
sort(sample(1:50, 6))
knit_with_parameters("C:/Users/mingt/Desktop/Programming_Practice/README.rmd")
getwd()
system("java -version")
# install.packages('sparklyr')
library(sparklyr)
library(DBI)
library(dplyr)
packageVersion('sparklyr')
spark_available_versions()
spark_installed_versions()
options(spark.install.dir = "installation- path")
# spark_uninstall(version = "3.0.3", hadoop = "2.7")
# spark_uninstall(version = "3.0.3", hadoop = "3.2")
spark_home_dir() # the path when using a local spark clister
sc <- spark_connect(master = 'local'
, version = "3.0.3")
getwd()
system("java -version")
# install.packages('sparklyr')
library(sparklyr)
library(DBI)
library(dplyr)
packageVersion('sparklyr')
spark_installed_versions()
# spark_uninstall(version = "3.0.3", hadoop = "2.7")
# spark_uninstall(version = "3.0.3", hadoop = "3.2")
spark_home_dir() # the path when using a local spark clister
sc <- spark_connect(master = 'local'
, version = "3.0.3")
# mtcars
cars <- copy_to(sc, mtcars)
cars
spark_web(sc)
dbGetQuery(sc, "select count(*) from mtcars")
str(cars)
typeof(cars)
class(cars)
select(cars, hp, mpg) %>%
sample_n(100) %>%
collect() %>%
plot()
select(cars, hp, mpg) %>%
sample_n(100) %>%
collect() %>%
str()
mod_1 <- ml_linear_regression(cars, mpg ~ hp)
mod_1
mod_2 <- mod_1 %>%
ml_predict(copy_to(sc, data.frame(hp = 250 + 10*1:10))) %>%
transmute(hp = hp, mpg = prediction)
mod_2
mod_2 %>% full_join(cars)
mod_2 %>% full_join(select(cars, hp, mpg))
mod_2 %>% full_join(select(cars, hp, mpg)) %>%
collect() %>% plot()
mod_2 %>% full_join(select(cars, hp, mpg)) %>%
collect() # %>% plot()
?spark_write_csv
spark_write_csv(cars, "cars.csv")
spark_write_csv(cars, "cars_2.csv")
cars2 <- spark_read_csv(sc, "cars.csv")
identical(cars, cars2)
cars
cars2
# install.packages('sparklyr.nested')
library(sparklyr.nested)
# sparklyr.nested::sdf_nest(cars, am)
sparklyr.nested::sdf_nest(cars, hp) %>%
group_by(cyl) %>%
summarise(data = collect_list(data))
# sparklyr.nested::sdf_nest(cars, am)
?sdf_nest
?spark_apply
cars %>% spark_apply(~round(.x)) # distribute R code across spark cluster
getwd()
write.csv(mtcars, "input/cars_1.csv", row.names = FALSE)
sc
stream2
stream2 <- stream_read_csv(sc, "input/") %>%
select(mpg, cyl, disp) %>%
stream_write_csv("output/")
dir("output", pattern =".csv")
stream2
write.csv(mtcars, "input/cars_2.csv", row.names = FALSE)
dir("output", pattern =".csv")
stream_stop(stream2)
spark_log(sc)
spark_log(sc, filter = 'sparklyr')
spark_log(sc, filter = 'Executor')
spark_log(sc, filter = 'RDD')
spark_disconnect_all()
# tdm_2_mat[row.names(tdm_2_mat) == 'just' ]
pbinom(50, 1000, 0.01)
?pbinom
# tdm_2_mat[row.names(tdm_2_mat) == 'just' ]
dbinom(50, 1000, 0.01)
# tdm_2_mat[row.names(tdm_2_mat) == 'just' ]
pbinom(10, 1000, 0.01)
# tdm_2_mat[row.names(tdm_2_mat) == 'just' ]
pbinom(10, 1000, 0.01)
pbinom(50, 1000, 0.01)
success <- 0:50
plot(success, dbinom(success, size=1000, prob=.01),type='h')
success2 <- 0:1000
plot(success2, dbinom(success, size=1000, prob=.01),type='h')
success2 <- 0:1000
plot(success2, dbinom(success, size=1000, prob=.01),type='h')
plot(success2, dbinom(success2, size=1000, prob=.01),type='h')
plot(success2, dbinom(success2, size=1000, prob=.01),type='h',
xlim = c(0, 50))
plot(success, dposs(success, size=1000, prob=.01),type='h')
plot(success, dpois(success, size=1000, prob=.01),type='h')
?dpois
plot(success, dpois(success, 10),type='h')
plot(success, dbinom(success, size=1000, prob=.01),type='h')
success2 <- 0:1000
success2 <- 0:1000
plot(success2, dbinom(success2, size=1000, prob=.01),type='h',
xlim = c(0, 50))
source("~/.active-rstudio-document", echo=TRUE)
library(nycflights13)
airlines
airports
flights
weather
airlines
write_csv(airlines, 'powerbi_ex_1_airlines')
write_csv(airlines, 'powerbi_ex_1_airlines.csv')
write_csv(airports, 'powerbi_ex_1_airports.csv')
write_csv(flights, 'powerbi_ex_1_flights.csv')
write_csv(weather, 'powerbi_ex_1_weather.csv')
write_csv(planes, 'powerbi_ex_1_planes.csv')
dnorm(230, 200, 15)
dnorm(230, 200, 15, FALSE)
dnorm(230, 200, 15, TRUE)
qnorm(0.25, 200, 15)
# options(spark.install.dir = "installation- path")
spark_install("2.3")
library(sparklyr)
# options(spark.install.dir = "installation- path")
spark_install("2.3")
spark_installed_versions()
library(janeaustenr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.3") #master: driver node; “main” machine from the Spark cluster
spark_config()
config <- spark_config()
config$spark.driver.cores   <- 2
config$spark.executor.cores <- 4
config$spark.executor.memory <- "4G"
# spark_config()
# config
sc <- spark_connect(master = "local", version = "2.3", config=config) #master: driver node; “main” machine from the Spark cluster
# install.packages('sparklyr')
library(sparklyr)
library(DBI)
library(dplyr)
# options(spark.install.dir = "installation- path")
# spark_install("2.3")
# spark_install("3.0")
# spark_uninstall(version = "3.0.3", hadoop = "2.7")
# spark_uninstall(version = "3.0.3", hadoop = "3.2")
spark_home_dir() # the path when using a local spark clister
library(janeaustenr)
library(dplyr)
config <- spark_config()
config$spark.driver.cores   <- 2
config$spark.executor.cores <- 4
config$spark.executor.memory <- "4G"
# spark_config()
# config
options(sparklyr.log.console = TRUE)
sc <- spark_connect(master = "local", version = "2.3", config=config) #master: driver node; “main” machine from the Spark cluster
sc <- spark_connect(master = "local", version = "2.3", config=config) #master: driver node; “main” machine from the Spark cluster
spark_installed_versions()
?pnorm
1- pnorm(4.375)
1- pnorm(1.75)
pnorm(-0.513/0.4)
1 - pnorm(0.6)
1 - pnorm(1.6)
tolower('APPLICATION & TECH SUPPORT SPECIALIST 1')
install.packages('rtools')
install.packages('rtools')
df_2 <- read_csv('database_project.csv')
setwd('C:\\Users\\mingt\\Downloads')
df_2 <- read_csv('database_project.csv')
library(readr)
df_2 <- read_csv('database_project.csv')
setwd('C:\\Users\\mingt\Documents\\db-capstone-project\\data\\landing')
setwd('C:\\Users\\mingt\Documents\\db-capstone-project\\data\\landing')
setwd('C:\\Users\\mingt\\Documents\\db-capstone-project\\data\\landing')
df_2 <- read_csv('database_project.csv')
df_2[1:82, ]
df_2[[1]]
distinct(df_2[[1]])
unique(df_2[[1]])
length(unique(df_2[[1]]))
